{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# library\n",
    "from elasticsearch import Elasticsearch\n",
    "\n",
    "import numpy as np\n",
    "import transformers\n",
    "from transformers import BertJapaneseTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#INDEX has the same role as Table in SQL.\n",
    "INDEX_ID = \"INDEX_ID\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create index\n",
    "def create_user(index_id):\n",
    "    es = Elasticsearch(\n",
    "        \"END POINT URL\",\n",
    "        http_auth=(\"USER_ID\", \"USER PASSWORD\")\n",
    "    )\n",
    "    # Describe the content and data type you want to save\n",
    "    # This mapping is example.\n",
    "    mapping = {\n",
    "        \"mappings\": {\n",
    "            \"properties\": {\n",
    "                \"timestamp\": {\"type\": \"date\"},\n",
    "                \"COLUMN_NAME_1\": {\"type\": \"text\"},\n",
    "                \"COLUMN_NAME_2\": {\"type\": \"text\"},\n",
    "                \"COLUMN_NAME_3\": {\"type\": \"long\"},\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    es.indices.create(index=index_id, body=mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fetch all data from index\n",
    "def fetch_all(index_id):\n",
    "    es = Elasticsearch(\n",
    "        \"END POINT URL\",\n",
    "        http_auth = (\"USER ID\", \"USER PASSWORD\")\n",
    "    )\n",
    "    query = {\n",
    "        \"query\": {\n",
    "            \"match_all\":{},\n",
    "        },\n",
    "        \"sort\": [{\"timestamp\":\"desc\"}]\n",
    "    }\n",
    "    result = es.search(index=user_id, body=query, size=10000)\n",
    "    output = []\n",
    "\n",
    "    # output like json\n",
    "    for document in result[\"hits\"][\"hits\"]:\n",
    "        temp = {}\n",
    "        temp[\"id\"] = document[\"_id\"]\n",
    "        temp[\"COLUMN_NAME_1\"] = document[\"_source\"][\"COLUMN_NAME_1\"]\n",
    "        temp[\"COLUMN_NAME_2\"] = document[\"_source\"][\"COLUMN_NAME_2\"]\n",
    "        output.append(t_output)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The big difference from other database services is that they can calculate cos similarity.\n",
    "# Our service use this feature for calculating text similarity with Bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bert\n",
    "\n",
    "class BertSequenceVectorizer:\n",
    "    def __init__(self):\n",
    "        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "        self.model_name = 'cl-tohoku/bert-base-japanese-whole-word-masking'\n",
    "        self.tokenizer = BertJapaneseTokenizer.from_pretrained(self.model_name)\n",
    "        self.bert_model = transformers.BertModel.from_pretrained(self.model_name)\n",
    "        self.bert_model = self.bert_model.to(self.device)\n",
    "        self.max_len = 128\n",
    "            \n",
    "\n",
    "    def vectorize(self, sentence : str) -> np.array:\n",
    "        inp = self.tokenizer.encode(sentence)\n",
    "        len_inp = len(inp)\n",
    "\n",
    "        if len_inp >= self.max_len:\n",
    "            inputs = inp[:self.max_len]\n",
    "            masks = [1] * self.max_len\n",
    "        else:\n",
    "            inputs = inp + [0] * (self.max_len - len_inp)\n",
    "            masks = [1] * len_inp + [0] * (self.max_len - len_inp)\n",
    "\n",
    "        inputs_tensor = torch.tensor([inputs], dtype=torch.long).to(self.device)\n",
    "        masks_tensor = torch.tensor([masks], dtype=torch.long).to(self.device)\n",
    "        \n",
    "        # seq_out, pooled_out = self.bert_model(inputs_tensor, masks_tensor)\n",
    "        seq_out = self.bert_model(inputs_tensor, masks_tensor).last_hidden_state\n",
    "\n",
    "        if torch.cuda.is_available():    \n",
    "            return seq_out[0][0].cpu().detach().numpy() # 0番目は [CLS] token, 768 dim の文章特徴量\n",
    "        else:\n",
    "            # print(seq_out[0][0])\n",
    "            return seq_out[0][0].detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# search with text similarity\n",
    "def search(self, target_text):\n",
    "        BSV = BertSequenceVectorizer()\n",
    "        query_vector = BSV.vectorize(target_text)\n",
    "\n",
    "        script_query = {\n",
    "            \"script_score\": {\n",
    "                \"query\": {\"match_all\": {}},\n",
    "                \"script\": {\n",
    "                    \"source\": \"cosineSimilarity(params.query_vector, doc['content_vector']) + 1.0\",\n",
    "                    \"params\": {\"query_vector\": query_vector}\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "\n",
    "        response = es.search(\n",
    "            index=INDEX_NAME,\n",
    "            body = {\n",
    "                \"size\": SEARCH_SIZE,\n",
    "                \"query\": script_query,\n",
    "                \"_source\": {\"includes\": [\"COLUMN_NAME_1\", \"COLUMN_NAME_2\"]}\n",
    "            }\n",
    "        )\n",
    "        result = []\n",
    "        for hit in response[\"hits\"][\"hits\"]:\n",
    "            t_dict = {}\n",
    "            print(\"id: {}, score: {}\".format(hit[\"_id\"], hit[\"_score\"]))\n",
    "            print(hit[\"_source\"][\"COLUMN_NAME_1\"])\n",
    "            t_dict[\"id\"] = hit[\"_id\"]\n",
    "            t_dict[\"score\"] = hit[\"_score\"]\n",
    "            t_dict[\"title\"] = hit[\"_source\"][\"COLUMN_NAME_1\"]\n",
    "            t_dict[\"content\"] = hit[\"_source\"][\"COLUMN_NAME_2\"]\n",
    "            result.append(t_dict)\n",
    "        \n",
    "        return result"
   ]
  }
 ]
}