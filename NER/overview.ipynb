{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "makePack.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "tOGvd0p1_umo"
      },
      "source": [
        "# !pip install -U ginza\n",
        "# import spacy\n",
        "# from spacy import displacy\n",
        "# from thinc.extra.load_nlp import get_spacy\n",
        "# import pkg_resources, imp\n",
        "# import pandas as pd\n",
        "# from sklearn.model_selection import train_test_split\n",
        "# import copy\n",
        "\n",
        "# # evaluate\n",
        "# from sklearn.metrics import precision_recall_fscore_support\n",
        "# from sklearn.metrics import classification_report\n",
        "\n",
        "# # minibatch\n",
        "# import random\n",
        "# from spacy.util import minibatch, compounding"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-v2ApVvME6qa"
      },
      "source": [
        "# train.py"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CjA3AE7OE7gX",
        "outputId": "94c89925-ac11-432b-a0f6-a5462ef59639"
      },
      "source": [
        "!pip install -U ginza\n",
        "import spacy\n",
        "from spacy import displacy\n",
        "from thinc.extra.load_nlp import get_spacy\n",
        "import pkg_resources, imp\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "import copy\n",
        "\n",
        "# evaluate\n",
        "from sklearn.metrics import precision_recall_fscore_support\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# minibatch\n",
        "import random\n",
        "from spacy.util import minibatch, compounding\n",
        "\n",
        "def _get_model():\n",
        "  imp.reload(pkg_resources)\n",
        "  nlp = get_spacy(\"ja_ginza\")\n",
        "  return nlp\n",
        "\n",
        "def conv_cat(cats, labels):\n",
        "  d = {}\n",
        "  for l in labels:\n",
        "    d[l] = False\n",
        "  temps = [copy.deepcopy(d) for i in range(len(cats))]\n",
        "  for i, label in enumerate(cats):\n",
        "    temps[i][label] = True\n",
        "  return temps\n",
        "\n",
        "def build_bow_text_classifier(\n",
        "    nr_class, ngram_size=1, exclusive_classes=False, no_output_layer=False, **cfg):\n",
        "  with Model.define_operators({\">>\": chain}):\n",
        "      model = with_cpu(\n",
        "      Model.ops, extract_ngrams(ngram_size, attr=ORTH) >> LinearModel(nr_class)\n",
        "  )\n",
        "  if not no_output_layer:\n",
        "      model = model >> (cpu_softmax if exclusive_classes else logistic)\n",
        "  model.nO = nr_class\n",
        "  return model\n",
        "\n",
        "def evaluate(tokenizer, textcat, docs, cats, verbose=False):\n",
        "    y_true = [max(cat.items(), key=lambda x:x[1])[0] for cat in cats]\n",
        "    y_pred = []\n",
        "    for i, doc in enumerate(textcat.pipe(docs)):\n",
        "        prediction = max(doc.cats.items(), key=lambda x:x[1])[0]\n",
        "        y_pred.append(prediction)\n",
        "    if verbose == False:\n",
        "      p, r, f1 = precision_recall_fscore_support(y_true, y_pred, average=\"micro\")[:3]    \n",
        "      return {\"textcat_p\": p, \"textcat_r\": r, \"textcat_f\": f1}\n",
        "    else:\n",
        "      return classification_report(y_true, y_pred)\n",
        "\n",
        "class Main:\n",
        "  def __init__(self):\n",
        "    self.model_path = \"./model/my_model\"\n",
        "    self.nlp = _get_model()\n",
        "  \n",
        "  def train(self, training_directory):\n",
        "    df = pd.read_csv(training_directory)\n",
        "    train_df, test_df = train_test_split(df, test_size = 0.4, random_state=0)\n",
        "    test_df, def_df = train_test_split(test_df, test_size=0.5, random_state=0)\n",
        "\n",
        "    train_texts = train_df['text']\n",
        "    train_cats = train_df['label']\n",
        "    dev_texts = dev_df['text']\n",
        "    dev_cats = dev_df['label']\n",
        "    test_texts = test_df['text']\n",
        "    test_cats = test_df['label']\n",
        "\n",
        "    labels = train_df['label'].unique()\n",
        "\n",
        "    train_cats = conv_cat(train_cats, labels)\n",
        "    dev_cats = conv_cat(dev_cats, labels)\n",
        "    test_cats = conv_cat(test_cats, labels)\n",
        "\n",
        "    train_texts = train_texts.values\n",
        "    dev_texts = dev_texts.values\n",
        "    test_texts = test_texts.values\n",
        "\n",
        "    train_docs = list(nlp.pipe(train_texts, disable=['ner']))\n",
        "    dev_docs = list(nlp.pipe(dev_texts, disable=['ner']))\n",
        "    test_docs = list(nlp.pipe(test_texts, disable=['ner']))\n",
        "\n",
        "    train_data = list(zip(train_docs, [{\"cats\": cats} for cats in train_cats]))\n",
        "\n",
        "    textcat = nlp.create_pipe(\"textcat\", config={\"exclusive_classes\": True, \"architecture\": \"bow\"})\n",
        "    nlp.add_pipe(textcat, last=True)\n",
        "    for label in train_df['label'].unique():\n",
        "      textcat.add_label(label)\n",
        "\n",
        "    other_pipes = [pipe for pipe in nlp.pipe_names if pipe != \"textcat\"]\n",
        "    n_iter = 20\n",
        "\n",
        "    with nlp.disable_pipes(*other_pipes):  # only train textcat\n",
        "        textcat = nlp.pipeline[-1][-1]\n",
        "        optimizer = textcat.begin_training() # NOTE\n",
        "        print(\"Training the model...\")\n",
        "        print(\"{:^5}\\t{:^5}\\t{:^5}\\t{:^5}\".format(\"LOSS\", \"P\", \"R\", \"F\"))\n",
        "        batch_sizes = compounding(4.0, 32.0, 1.001)\n",
        "        num_samples = len(train_data)\n",
        "        for i in range(n_iter):\n",
        "            losses = {}\n",
        "            # batch up the examples using spaCy's minibatch\n",
        "            random.shuffle(train_data)\n",
        "            batches = minibatch(train_data, size=batch_sizes)\n",
        "            processed = 0\n",
        "            for i, batch in enumerate(batches):\n",
        "                texts, annotations = zip(*batch)\n",
        "                nlp.update(texts, annotations, sgd=optimizer, drop=0.2, losses=losses)\n",
        "                processed += len(batch)\n",
        "                percentage = processed / num_samples * 100.0\n",
        "                #if i % 20 == 0:\n",
        "                #  print(\"  %5.2f %% of epoch done. batch size = %d\" % (percentage, len(batch)))\n",
        "            with textcat.model.use_params(optimizer.averages):\n",
        "                # evaluate on the dev data split off in load_data()\n",
        "                scores = evaluate(nlp.tokenizer, textcat, dev_docs, dev_cats)\n",
        "            #print(\"{:^5}\\t{:^5}\\t{:^5}\\t{:^5}\".format(\"LOSS\", \"P\", \"R\", \"F\"))\n",
        "            print(\n",
        "                \"{0:.3f}\\t{1:.3f}\\t{2:.3f}\\t{3:.3f}\".format(  # print a simple table\n",
        "                    losses[\"textcat\"],\n",
        "                    scores[\"textcat_p\"],\n",
        "                    scores[\"textcat_r\"],\n",
        "                    scores[\"textcat_f\"],\n",
        "                )\n",
        "            )\n",
        "    with textcat.model.use_params(optimizer.averages):\n",
        "      report = evaluate(nlp.tokenizer, textcat, test_docs, test_cats, verbose=True)\n",
        "      print(\"test loss = %5.3f\\n\" % (losses[\"textcat\"]))\n",
        "      print(report)\n",
        "    \n",
        "    nlp.to_disk(\"./model/new_model\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already up-to-date: ginza in /usr/local/lib/python3.6/dist-packages (4.0.5)\n",
            "Requirement already satisfied, skipping upgrade: SudachiDict-core>=20200330; python_version >= \"3.5\" in /usr/local/lib/python3.6/dist-packages (from ginza) (20201223.post1)\n",
            "Requirement already satisfied, skipping upgrade: spacy<3.0.0,>=2.3.2 in /usr/local/lib/python3.6/dist-packages (from ginza) (2.3.5)\n",
            "Requirement already satisfied, skipping upgrade: ja-ginza<4.1.0,>=4.0.0 in /usr/local/lib/python3.6/dist-packages (from ginza) (4.0.0)\n",
            "Requirement already satisfied, skipping upgrade: SudachiPy>=0.4.9; python_version >= \"3.5\" in /usr/local/lib/python3.6/dist-packages (from ginza) (0.5.1)\n",
            "Requirement already satisfied, skipping upgrade: setuptools in /usr/local/lib/python3.6/dist-packages (from spacy<3.0.0,>=2.3.2->ginza) (51.3.3)\n",
            "Requirement already satisfied, skipping upgrade: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.6/dist-packages (from spacy<3.0.0,>=2.3.2->ginza) (2.23.0)\n",
            "Requirement already satisfied, skipping upgrade: blis<0.8.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy<3.0.0,>=2.3.2->ginza) (0.4.1)\n",
            "Requirement already satisfied, skipping upgrade: numpy>=1.15.0 in /usr/local/lib/python3.6/dist-packages (from spacy<3.0.0,>=2.3.2->ginza) (1.19.5)\n",
            "Requirement already satisfied, skipping upgrade: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy<3.0.0,>=2.3.2->ginza) (1.1.3)\n",
            "Requirement already satisfied, skipping upgrade: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy<3.0.0,>=2.3.2->ginza) (1.0.5)\n",
            "Requirement already satisfied, skipping upgrade: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy<3.0.0,>=2.3.2->ginza) (2.0.5)\n",
            "Requirement already satisfied, skipping upgrade: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy<3.0.0,>=2.3.2->ginza) (3.0.5)\n",
            "Requirement already satisfied, skipping upgrade: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy<3.0.0,>=2.3.2->ginza) (1.0.5)\n",
            "Requirement already satisfied, skipping upgrade: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy<3.0.0,>=2.3.2->ginza) (0.8.1)\n",
            "Requirement already satisfied, skipping upgrade: thinc<7.5.0,>=7.4.1 in /usr/local/lib/python3.6/dist-packages (from spacy<3.0.0,>=2.3.2->ginza) (7.4.5)\n",
            "Requirement already satisfied, skipping upgrade: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.6/dist-packages (from spacy<3.0.0,>=2.3.2->ginza) (1.0.0)\n",
            "Requirement already satisfied, skipping upgrade: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.6/dist-packages (from spacy<3.0.0,>=2.3.2->ginza) (4.41.1)\n",
            "Requirement already satisfied, skipping upgrade: dartsclone~=0.9.0 in /usr/local/lib/python3.6/dist-packages (from SudachiPy>=0.4.9; python_version >= \"3.5\"->ginza) (0.9.0)\n",
            "Requirement already satisfied, skipping upgrade: sortedcontainers~=2.1.0 in /usr/local/lib/python3.6/dist-packages (from SudachiPy>=0.4.9; python_version >= \"3.5\"->ginza) (2.1.0)\n",
            "Requirement already satisfied, skipping upgrade: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.0.0,>=2.3.2->ginza) (1.24.3)\n",
            "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.0.0,>=2.3.2->ginza) (2020.12.5)\n",
            "Requirement already satisfied, skipping upgrade: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.0.0,>=2.3.2->ginza) (3.0.4)\n",
            "Requirement already satisfied, skipping upgrade: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.0.0,>=2.3.2->ginza) (2.10)\n",
            "Requirement already satisfied, skipping upgrade: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy<3.0.0,>=2.3.2->ginza) (3.4.0)\n",
            "Requirement already satisfied, skipping upgrade: Cython in /usr/local/lib/python3.6/dist-packages (from dartsclone~=0.9.0->SudachiPy>=0.4.9; python_version >= \"3.5\"->ginza) (0.29.21)\n",
            "Requirement already satisfied, skipping upgrade: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy<3.0.0,>=2.3.2->ginza) (3.7.4.3)\n",
            "Requirement already satisfied, skipping upgrade: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy<3.0.0,>=2.3.2->ginza) (3.4.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JpHO_7JcEzfv"
      },
      "source": [
        "# main.py"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xQfh1PQfEy7d",
        "outputId": "a1831383-da24-4576-c087-153dc882d1dd"
      },
      "source": [
        "import spacy\n",
        "import pandas as pd\n",
        "\n",
        "class Main:\n",
        "  def __init__(self):\n",
        "    self.nlp = spacy.load(\"./model/new_model\")\n",
        "  def predict(self, texts):\n",
        "    docs = self.nlp.pipe(texts)\n",
        "    result = []\n",
        "    for i, doc in enumerate(docs):\n",
        "      prediction = max(doc.cats.items(), key=lambda x:x[1])[0]\n",
        "      ne_text = []\n",
        "      ne_label = []\n",
        "      for ent in doc.ents:\n",
        "        if ent.text in ne_text and ent.label_ in ne_label:\n",
        "          pass\n",
        "        else:\n",
        "          ne_text.append(ent.text)\n",
        "          ne_label.append(ent.label_)\n",
        "      result += [[prediction, ne_text, ne_label]]\n",
        "    return result\n",
        "\n",
        "if __name__ == '__main__':\n",
        "  temp = pd.read_csv(\"./test.csv\")\n",
        "  m = Main()\n",
        "  predict = m.predict(temp[\"text\"].values)\n",
        "  print(predict)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[['movie-enter', ['——', 'ホワイト・カラー', 'ティム・ディケイ', 'ピーター', 'バーク', 'ニール', 'ふたり', '俳優', 'ティム', 'プロデューサー', 'ボマー', '銃弾', 'アメリカ', '私の家庭では妻がズボンを履く', 'エリザベス', '犬', 'サッチモ', 'FBI', 'ナーバス', '脚本家', '愛妻家', '敏腕捜査官', 'ハニー', '人間', 'ひとつ', '両極端', '——シーズン2', 'シーズン2', '先生', 'シーズン3', '巡業', '2時間', '編集者', 'カメラマン', '1日', 'スタッフ', 'ウィリー', 'ティファニー・ティーセン', 'ティファニー', '１話', '７日間', 'ふたつ', '音声さん', 'ホワイトカラー “知的”犯罪ファイル'], ['Ordinal_Number', 'Book', 'Person', 'Person', 'Person', 'Person', 'N_Person', 'Position_Vocation', 'Person', 'Position_Vocation', 'Person', 'Animal_Part', 'Country', 'Book', 'Person', 'Mammal', 'Person', 'Position_Vocation', 'Person', 'Position_Vocation', 'Flora', 'Position_Vocation', 'Person', 'Mammal', 'Countx_Other', 'Animal_Disease', 'Date', 'Ordinal_Number', 'Position_Vocation', 'Ordinal_Number', 'Person', 'Period_Time', 'Position_Vocation', 'Position_Vocation', 'Period_Day', 'Position_Vocation', 'Person', 'Person', 'Product_Other', 'Ordinal_Number', 'Period_Day', 'N_Product', 'Position_Vocation', 'Product_Other']], ['sports-watch', ['15日', 'DeNA', '中村紀洋', 'サヨナラ2ランホームラン', '巨人', 'ナゴヤドーム', '中日', '3連戦', '5位', '原', '監督', 'DeNA戦', '4番', '阿部', 'サヨナラ', '中村', '18日', '5回', '2失点', 'ホールトン', '19日', '元中日監督', '落合博満', '氏', '選手', '落合', '東野', '亀井', 'コーチ', '先発', '脇腹痛', '1回', '20日', 'ヤクルト'], ['Date', 'Pro_Sports_Organization', 'Person', 'Person', 'Pro_Sports_Organization', 'Sports_Facility', 'Pro_Sports_Organization', 'N_Event', 'Rank', 'Person', 'Position_Vocation', 'Sports_League', 'Numex_Other', 'Person', 'Person', 'Person', 'Date', 'N_Event', 'Point', 'Name_Other', 'Date', 'Position_Vocation', 'Person', 'Title_Other', 'Position_Vocation', 'Person', 'Person', 'Person', 'Position_Vocation', 'Position_Vocation', 'Animal_Disease', 'Frequency', 'Date', 'Pro_Sports_Organization']], ['peachy', ['肌', 'アルビオン', '300名', '77％', '約8割', '71％', '50％', '白い', '39％', '93％', 'キメ', '2月18日', 'シフォンファンデーション', 'パーフェクト サマーシフォン', '毛穴', 'シフォン', 'パーフェクトサマーシフォンプライマー', 'パーフェクトサマーシフォン', '11g', '6色', 'パーフェクトサマーシフォン プライマー', '30g', '3,150', 'アルビオン\\u3000', 'ホワイト', 'パーフェクト\\u3000サマーシフォン'], ['Animal_Part', 'Person', 'N_Person', 'Percent', 'Percent', 'Percent', 'Percent', 'Nature_Color', 'Percent', 'Percent', 'Living_Thing_Part_Other', 'Date', 'Product_Other', 'Occasion_Other', 'Animal_Part', 'Dish', 'Music', 'Occasion_Other', 'Doctrine_Method_Other', 'Countx_Other', 'Occasion_Other', 'Weight', 'Product_Other', 'Person', 'Sports_Facility', 'Broadcast_Program']]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yI8H2ed9WUou",
        "outputId": "60b6cf9e-a4bb-48e3-e4a5-41a23bf11adf"
      },
      "source": [
        "predict[0]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['movie-enter',\n",
              " ['——',\n",
              "  'ホワイト・カラー',\n",
              "  'ティム・ディケイ',\n",
              "  'ピーター',\n",
              "  'バーク',\n",
              "  'ニール',\n",
              "  'ふたり',\n",
              "  '俳優',\n",
              "  'ティム',\n",
              "  'プロデューサー',\n",
              "  'ボマー',\n",
              "  '銃弾',\n",
              "  'アメリカ',\n",
              "  '私の家庭では妻がズボンを履く',\n",
              "  'エリザベス',\n",
              "  '犬',\n",
              "  'サッチモ',\n",
              "  'FBI',\n",
              "  'ナーバス',\n",
              "  '脚本家',\n",
              "  '愛妻家',\n",
              "  '敏腕捜査官',\n",
              "  'ハニー',\n",
              "  '人間',\n",
              "  'ひとつ',\n",
              "  '両極端',\n",
              "  '——シーズン2',\n",
              "  'シーズン2',\n",
              "  '先生',\n",
              "  'シーズン3',\n",
              "  '巡業',\n",
              "  '2時間',\n",
              "  '編集者',\n",
              "  'カメラマン',\n",
              "  '1日',\n",
              "  'スタッフ',\n",
              "  'ウィリー',\n",
              "  'ティファニー・ティーセン',\n",
              "  'ティファニー',\n",
              "  '１話',\n",
              "  '７日間',\n",
              "  'ふたつ',\n",
              "  '音声さん',\n",
              "  'ホワイトカラー “知的”犯罪ファイル'],\n",
              " ['Ordinal_Number',\n",
              "  'Book',\n",
              "  'Person',\n",
              "  'Person',\n",
              "  'Person',\n",
              "  'Person',\n",
              "  'N_Person',\n",
              "  'Position_Vocation',\n",
              "  'Person',\n",
              "  'Position_Vocation',\n",
              "  'Person',\n",
              "  'Animal_Part',\n",
              "  'Country',\n",
              "  'Book',\n",
              "  'Person',\n",
              "  'Mammal',\n",
              "  'Person',\n",
              "  'Position_Vocation',\n",
              "  'Person',\n",
              "  'Position_Vocation',\n",
              "  'Flora',\n",
              "  'Position_Vocation',\n",
              "  'Person',\n",
              "  'Mammal',\n",
              "  'Countx_Other',\n",
              "  'Animal_Disease',\n",
              "  'Date',\n",
              "  'Ordinal_Number',\n",
              "  'Position_Vocation',\n",
              "  'Ordinal_Number',\n",
              "  'Person',\n",
              "  'Period_Time',\n",
              "  'Position_Vocation',\n",
              "  'Position_Vocation',\n",
              "  'Period_Day',\n",
              "  'Position_Vocation',\n",
              "  'Person',\n",
              "  'Person',\n",
              "  'Product_Other',\n",
              "  'Ordinal_Number',\n",
              "  'Period_Day',\n",
              "  'N_Product',\n",
              "  'Position_Vocation',\n",
              "  'Product_Other']]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 48
        }
      ]
    }
  ]
}